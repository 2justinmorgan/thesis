
\chapter{Introduction}\label{ch:introduction}
This thesis paper outlines research of web bot and botnet detection schemes, addresses their strengths and weaknesses, as well as introduces a novel approach to detect bots and botnet attacks.
The work in this thesis includes an unsupervised machine learning algorithm to solve the dilemma of detecting sophisticated web bots masqueraded as humans. This section will provide a context of the dilemma and explain the motivation behind the novel approach. %, as well as list the scientific contributions.

\section{Context}\label{sec:context}
Web bots, otherwise known as bots, are human-imitating programs created with the intention of reducing the amount of simple and repetitive tasks a human would have to do, while performing these tasks at a speed much faster than a human can perform.
Some examples of these tasks may include searching for specific items, with a given criteria, through many items on an ecommerce site such as Amazon.
Another example can be the action of downloading lists textual data on a web page that would require the user to do so manually.
Search engines use bots, also known as search engine bots, crawlers or spiders, to index, or traverse and access, web pages of a website, storing information seen during these traversals, to later be referenced in a user's search.
These search engine bots, as well as link checkers, monitoring bots, and feed fetchers, are examples of "good" bots permitted by website administrators, via a robots.txt file, to traverse a website~\cite{ROVETTA2020102577}.

Similarly, a botnet is a collection of bots working in unison.
These bot collections are hosted by either multiple computers belonging to the botnet administrator, or more commonly, malware-infected computers belonging to victims of a botnet attack.
In the latter case, a botnet is controlled by a "botmaster" and used as a means to conduct network and browser attacks such as distributed denial-of-sevice (DDoS) attacks, as well as fraudulent activities such as spam, phishing, identity theft, and information exfiltration~\cite{inproceedings}.
The "net" portion in the "botnet" name derives from method of communication among the botmaster and bots in the botnet.

A bad bot, or "bot" in the context of this paper, present a problem when they scrape, or index and extract, data from websites without permission with the intention of reusing it, for example pricing or inventory levels, to gain a competitive advantage~\cite{bad_bot_report}.
This, however, does not imply that "good" bots are always good.
A primary reason why these seemingly benign bots may be problematic is their high intensity nature.
The presence of "good" bots on a website can skew analytics reports, thus falsely representing the popularity of certain pages of a website.
Therefore, being able to separate webisite traffic generated by human users and either type of bots, is essential for making business decisions~\cite{bad_bot_report}.
Regardless, bots can be used maliciously and irresponsibly, thus introducing a number of problems for the users and administrators of a website~\cite{1ee426975c3d46d2ba6ef5c2d76384c5}~\cite{bad_bot_report}.
Efforts to detect these bots have proven to be successful~\cite{akamai_bot_detection}~\cite{Hamidzadeh2018}~\cite{ZABIHIMAYVAN2017129}.
However, due to the increasing sophistication of bots, said detection schemes are often bypassed and deemed obsolete~\cite{ROVETTA2020102577}~\cite{STEVANOVIC2013698}~\cite{10.1109/DSN.2013.6575366}.

\section{Behavioral User Metrics}\label{sec:behavrioral-user-metrics}
When using a destop or laptop device, humans use a mouse or touchpad peripheral device to interact and navigate the graphical user interface.
During this navigation, a human user applies their own unique strategy and style, thus leaving traces or signature traits pertaining to and identifying that user.
Feature profiles generated, by human computer interaction-based researchers, are used and quantified in attempt to successfully differentiate and verify users~\cite{human_computer_interaction_based_intrusion_detection}.
This thesis work applies a similar strategy of differentiating users.
Since users are differentiable based on their behavioral metrics, and since sophisticated web bots are closely mimicking the behavior of human users, differentiating users by clustering the feature profiles of their behavioral metrics should be a means to separate users, bot or not, without any need for labeling or precursory knowledge of the behavior of potential bot users.
Users, or the feature metrics pertaining to a user's session, are differentiated into separate clusters that provide website administrators a user-specific network request log that is not IP address-specific.
Having this ability enables website administrators to make more informed decisions about the botness of users, or the clusters of similar behavioral metrics.

Human computer interaction-based biometrics researchers verify users by classifying their mouse and keyboard behavior~\cite{human_computer_interaction_based_intrusion_detection}.
Additionally, the user provides these biometrics inadvertently, without interfering with the UX experience or causing user friction.

\section{Motivation for Clustering}\label{sec:motivation-for-clustering}
Distil Networks classifies ~\cite{bad_bot_report} bot sophistication levels as follows:
\begin{itemize}
    \item \textbf{Simple}
        Connecting from a single, ISP-assigned IP address, this type connects to websites using automated scripts, not browers, and doesn't self-report, or masquerade, as being a real browser.
    \item \textbf{Moderate}
        Being more complex, this type uses "headless browser" software that simulates browser technology, including the ability to execute JavaScript code.
    \item \textbf{Sophisticated}
        Producing mouse movements and clicks that fool even sophisticated detection methods, these bad bots mimic human behavior and are the most evasive. They use browser automation software, or malware installed within real browsers, to connect to websites.
    \item \textbf{Advanced Persistent Bots (APBs)}
        APBs are a combination of moderate and sophisticated bad bots. They tend to cycle through random IP addresses, enter through anonymous proxies and peer-to-peer networks, and are able to change their user agents. They use a mix of technologies and methods to evade detection while maintaining persistency on target websites.
\end{itemize}
This thesis presents a novel approach to detect bots classified as either sophisticated or advanced persistent.
The objective of this approach is to develop a detection algorithm that is effective irrespective to the level of sophistication a bot masquerades a human user.
By clustering the behavioral metrics of users, or visitors of a website, detecting of the "botness" of said users will be similar to the nature of how users are tracked on websites that require login credentials upon entry.
If users can be differentiated based solely on their behavior, and not by relying on the client and IP info available in the server logs, than said users can be monitored as if they were logged-in to the website.
Having the ability to monitor users at this micro-level, without bias of any IP or proxy anonymity, enables website administrators to make more informed decisions about the web botness of users.
﻿
