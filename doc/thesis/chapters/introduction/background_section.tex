
\section{Background}\label{sec:background}
Web bots, otherwise known as bots, are human-imitating programs created with the intention of reducing the amount of simple and repetitive tasks a human would have to do, while performing these tasks at a speed much faster than a human can perform.
Some examples of these tasks may include searching for specific items, with a given criteria, through many items on an ecommerce site such as Amazon.
Another example can be the action of downloading lists textual data on a web page that would require the user to do so manually.
Search engines use bots, also known as search engine bots, crawlers or spiders, to index, or traverse and access, web pages of a website, storing information seen during these traversals, to later be referenced in a user's search.
These search engine bots, as well as link checkers, monitoring bots, and feed fetchers, are examples of "good" bots permitted by website administrators, via a robots.txt file, to traverse a website~\cite{ROVETTA2020102577}.

Similarly, a botnet is a collection of bots working in unison.
These bot collections are hosted by either multiple computers belonging to the botnet administrator, or more commonly, malware-infected computers belonging to victims of a botnet attack.
In the latter case, a botnet is controlled by a "botmaster" and used as a means to conduct network and browser attacks such as distributed denial-of-sevice (DDoS) attacks, as well as fraudulent activities such as spam, phishing, identity theft, and information exfiltration~\cite{inproceedings}.
The "net" portion in the "botnet" name derives from method of communication among the botmaster and bots in the botnet.

A bad bot, or "bot" in the context of this paper, present a problem when they scrape, or index and extract, data from websites without permission with the intention of reusing it, for example pricing or inventory levels, to gain a competitive advantage~\cite{bad_bot_report}.
This, however, does not imply that "good" bots are always good.
A primary reason why these seemingly benign bots may be problematic is their high intensity nature.
The presence of "good" bots on a website can skew analytics reports, thus falsely representing the popularity of certain pages of a website.
Therefore, being able to separate webisite traffic generated by human users and either type of bots, is essential for making business decisions~\cite{bad_bot_report}.
Regardless, bots can be used maliciously and irresponsibly, thus introducing a number of problems for the users and administrators of a website~\cite{1ee426975c3d46d2ba6ef5c2d76384c5}~\cite{bad_bot_report}.
Efforts to detect these bots have proven to be successful~\cite{akamai_bot_detection}~\cite{Hamidzadeh2018}~\cite{ZABIHIMAYVAN2017129}.
However, due to the increasing sophistication of bots, said detection schemes are often bypassed and deemed obsolete~\cite{ROVETTA2020102577}~\cite{STEVANOVIC2013698}~\cite{10.1109/DSN.2013.6575366}.

