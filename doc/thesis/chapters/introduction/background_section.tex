
\section{Background}\label{sec:background}

\subsection{Bots and Botnets}\label{subsec:bots-and-botnets}
Web bots, otherwise known as bots, are human-imitating programs created with the intention of reducing the amount of simple and repetitive tasks a human would have to do, while performing these tasks at a speed much faster than a human can perform.
Some examples of these tasks may include searching for specific items, with a given criteria, through many items on an ecommerce site such as Amazon.
Another example can be the action of downloading lists textual data on a web page that would require the user to do so manually.
Search engines use bots, also known as search engine bots, crawlers or spiders, to index, or traverse and access, web pages of a website, storing information seen during these traversals, to later be referenced in a user's search.
These search engine bots, as well as link checkers, monitoring bots, and feed fetchers, are examples of "good" bots permitted by website administrators, via a robots.txt file, to traverse a website~\cite{ROVETTA2020102577}.

Similarly, a botnet is a collection of bots working in unison.
These bot collections are hosted by either multiple computers belonging to the botnet administrator, or more commonly, malware-infected computers belonging to victims of a botnet attack.
In the latter case, a botnet is controlled by a "botmaster" and used as a means to conduct network and browser attacks such as distributed denial-of-sevice (DDoS) attacks, as well as fraudulent activities such as spam, phishing, identity theft, and information exfiltration~\cite{inproceedings}.
The "net" portion in the "botnet" name derives from method of communication among the botmaster and bots in the botnet.

The~\cite{bad_bot_report} report states that a "bad" bot, or "bot" in the context of this paper, presents problems when they scrape, or index and extract, data from websites without permission with the intention of reusing it, for example pricing or inventory levels, to gain a competitive advantage.
This, however, does not imply that "good" bots, as described in~\cite{bad_bot_report}, are always good.
A primary reason why these seemingly benign bots may be problematic is their high intensity nature.
The presence of "good" bots on a website can skew analytics reports, thus falsely representing the popularity of certain pages of a website.
Therefore, being able to separate webisite traffic generated by human users and either type of bots, is essential for making business decisions~\cite{bad_bot_report}.
Regardless, bots can be used maliciously and irresponsibly, thus introducing a number of problems for the users and administrators of a website~\cite{1ee426975c3d46d2ba6ef5c2d76384c5}~\cite{bad_bot_report}.
Efforts to detect these bots have proven to be successful~\cite{akamai_bot_detection}~\cite{Hamidzadeh2018}~\cite{ZABIHIMAYVAN2017129}.
However, due to the increasing sophistication of bots, said detection schemes are often bypassed and deemed obsolete~\cite{ROVETTA2020102577}~\cite{STEVANOVIC2013698}~\cite{10.1109/DSN.2013.6575366}.

\subsection{Supervised and Unsupervised Learning Methods}\label{subsec:supervised-and-unsupervised-learning-methods}
Machine learning is a widely used term that implies the autonomous improvement of algorithms as they are invoked, hence the learning aspect.
Two common methods of these algorithms implement what is known as supervised and unsupervised learning.
Although both methods autonomously improve in accuracy and efficiency as they are used, supervised learning requires a labeled dataset as a source of truth whereas unsupervised learning finds patterns and meaning in a non-labeled dataset.

Labels identify a datapoint for a supervised learning implementation to compare against an inputted, non-labeled datapoint.
If the labeled datapoints is not representative of a typical inputted, non-labeled datapoint, the supervised learning implementation will lack accuracy.
Therefore, a supervised learning implementation is as effective as the robustness of its labeled dataset.
An example of a datapoint with a label would be a series of behavioral features that are represented, or labeled, with a username.
If an inputted, non-labeled datapoint closely resembles that of a datapoint(s) with a specific user's username label, then there is an high probability that the inputted, non-labeled datapoint originated from that specific user.

An unsupervised learning implementation does not use or require labels in a dataset.
Instead, information and meaning of a dataset is interpreted through patterns in the data.
Therefore, an unsupervised learning implementation is as effective as the methods used to identify and interpret patterns in a dataset.
In the context of this work, features are extracted from a user's mouse movement behavior, and unique patterns of that user are identified and leveraged.

\textbf{Classification} is the identification of a datapoint type, or class, that is represented by a label in the labeled dataset.
Classification is an outcome of a supervised learning implementation.
\textbf{Clustering} is the identification of patterns in dataset and the grouping of the datapoints, by their patterns, in that dataset.
Clustering is an outcome of an unsupervised learning implementation.
