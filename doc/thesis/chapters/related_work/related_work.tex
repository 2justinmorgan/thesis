
\chapter{Related Work}\label{ch:related-work}
The unsupervised machine learning approach of this thesis research was inspired by a few preexisting implementations of the like.
By observing the strengths and weaknesses of these implementations, the outcome of this work is expected to be an improvement and beneficial contribution to the field of web bot and botnet detection research.
This section will provide a synopsis and evaluation of a few of the related work that provided inspiration to this work.

\section{Supervised Learning}\label{sec:supervised-learning}
There are several bot detection schemes that utilize labeled datasets to train models in their implementation.
\subsection{Graphs and CNNs}\label{subsec:graphs-and-cnns}
BotGraph is one which represented the sitemap, or order of page indexing, of a user with a nodes-and-edges graph.
Since these generated graphs are images that represent a number of metrics based on user behavior, not user identification, the research utilized convoluted neural networks (CNN)s to predict users types of either bot or human.
The BotGraph research concludes that this approach yields about a 95\% success rate in detecting bots.

This article begins by introducing key concepts mentioned by a company that specializes in web bot detection, ShieldSquare, the difference of identity-based and behavior-based bot detection.
The identify-based method utilizes client-side JavaScript to collect parameters like browser fingerprints; a collection of information about your browser type and version, as well as your operating system, active plugins, timezone, language, screen resolution and various other active settings~\cite{browser_fingerprinting}.
Whereas the behavior-based refers to the number pages visited per session, the duration of time per page visit, the number of page revisits, etc.
This research more closely resembles the behavior-based method but instead represents such metrics via a graph image which, according to the research, contains more unique features per user.
The paper continues by introducing a violator blacklist, biometric data validation like scroll and mouse movement method by Distil Networks, as well as the UserAgent variable present in HTTP protocol; an unstable bot-detection method as more advanced bots can falsify their identity by simply hiding or modifying the UserAgent variable.
Deep Defense introduced a recurrent neural network (RNN)-based model that takes as input sameshape segment splits from the web server access logs, then encoded the request information in each line of segment to a numerical matrix.
The paper states that, while this method is most similar to the outlined research implementation, the inference efficiency of Deep Defense relies too heavily on the length of the same-shape segments; in which BotGraph supposedly proves to be more stable.

The research implementation included BotGraph, a program that generates graph-based representations of users' sitemap traversals.
Since these graphs were in image format, the implementation employed convolutional neural network (CNN) inferences to distinguish bots from human user types.
The details of BotGraph are as follows:
\begin{itemize}
    \item request - timestamp, HTTP method, request URI, status, host IP, user{\_}agent, client IP variables
    \item session - a method if identifying a series of client requests (by bot or human)
    \item identity - user{\_}agent and client IP variables for the client, and host IP variable for the server
    \item behavior - request URI and status variables for the access frequency metric per graph node.
\end{itemize}
The graph can be described as $G = (V, E)$:
\begin{itemize}
    \item $G$: a directed graph
    \item $V$: set of nodes representing all same-pattern URLs visited, i.e \textbf{/page?id=3} is pattern \textbf{/page?id=*}
    \item $E$: set of directed edges, each representing access points, i.e. mutliple \textbf{a} tag elements with same href
\end{itemize}
Below is a figure of the BotGraph architecture. As you can see, BotGraph runs in a three-step process:
\begin{enumerate}
    \item Build a sitemap through one of three methods: \textit{active crawling, passive sniffing, self providing}
    \begin {enumerate}
        \item \textit{active crawling}: crawling typically starts from website homepage and recursively enters each hyperlink from the current page
        \item \textit{passive sniffing}: the urls of a websiteâ€™s traffic are monitored, learned and then used to build the sitemap. This is a less intrusive alternative to active crawling
        \item \textit{self providing}: the site provides its own sitemap for bot detection. This is the most accurate
    \end{enumerate}
    \item Map requests listed in server access logs to denote sessions as subgraphs in a sitemap
    \item Generate 2-dimensional trace images, translating a bot detection task into an image recognition
\end{enumerate}
\begin{figure}[!h]
    \includegraphics[width=1\columnwidth]{figures/BotGraph_fig1}
    \caption{Architecture of BotGraph}
    \label{fig:botgraph}
\end{figure}
This implementation used a model trained on a data set generated by 30+ professionals that manually tagged web traffic via JavaScript support checking, mouse movement and click tracking, IP reputation, UserAgent blacklisting.
BotGraph is comparable to other bot detection methods such as long short-term memory, SVM, XGBoost, AdaBoost, decision tree (DT), random forest (RF), multi-layer perceptron (MLP), etc.

A weakness in the BotGraph method of bot detection is when a user visits a lower number pages per session, i.e. less than 3 visits.
This is because bot and human users have too similar browsing behavior, namely their session sitemap traversal, creating near-identical sitemap graphs.
However, this research claims that BotGraph is a very effective and efficient method of detecting bots as it achieves about 95\% in precision and recall while relying only on the client's behavior and not the client's identity variables.
Some needed improvements include an implementation that generates more detailed graph-related features to better describe the characteristics of user sessions, specifically to identify the behavior of web bots.

\subsection{Training Data Generation and Semi-Supervised Learning}\label{subsec:training-data-generation-and-semi-supervised-learning}
The~\cite{bot_detection_for_search_engines} research addresses the common problem with supervised learning-based bot detection schemes.
Due to the high traffic of modern search engines, it is infeasible to rely on human judges to generate labeled datasets, used in supervised learning approaches, by manually inspecting the search logs to label bot and human users.
On a controlled webserver environment, labeled datasets were created by analyzing the response and activity of CAPTCHA challenges sent to the users.
In an effort to enhance the user experience, challenges were sent selectively, either when the webserver is experiencing a high volume of network traffic, or when a user makes a high number of requests in a short amount of time.
When presented with a challenge, the user can either disregard the challenge by exiting the session, answer correctly, or answer correctly, thus answering "no response", "correct response", or "wrong response", respectively.
\begin{figure}[!h]
    \centering
    \includegraphics[width=1\columnwidth]{figures/semi_supervised_CAPTCHA_training_data_generation_results}
    \caption{User answers to CAPTCHA challenges}
    \label{fig:captcha-user-answers}
    {\small The research notes that since the users were selected non-uniformly, most answers to the challenge were "no response"}
\end{figure}
About 80\% of the received responses were correct.
This accounted for the majority of the training data with "human" labels.
The remaining "human" labels were pulled from the large set of "no response" answers by analyzing heuristics of the user's number of clicks in a time period, the number of search result pages browsed, as well as information of the user's IP address.
Users were labeled "bot" if the user's answer was "no response", and the user did not satisfy thresholds of the heuristics previously described.
This CAPTCHA challenge-method accounted for the "0-cost" training data generation method, as described in the research.

From the creation of the labeled training dataset, the following features were extracted from the users:
\begin{itemize}
    \item \textbf{PageTrackedCount}: measures the number of pages that the user browses
    \item \textbf{UserClickCount}: measures the number of mouse clicks on the search result pages
    \item \textbf{AllHitCount}: measures the overall "impressions" that the user receives in addition to the search results
    \item \textbf{UserUniqueIPs}: measures the unique number of IPs a user is using
    \item \textbf{UserUniqueQueries}: measures the unique number of queries issued by a single user in a search session
    \item \textbf{Blacklisting Rules}:
    \begin{enumerate}
        \item \textbf{Form}: triggered when a user includes in the query certain obscure codes that are designed mostly for internal search engine instrumentation purposes that should be unfamiliar to most genuine human users
        \item \textbf{IP}: a list of IPs that are publicly identified as Internet spammers and labeled all the traffic from these IPs as "bot"
        \item \textbf{Query}:  this rule is triggered when the query composition is too complicated to be manually typed in by a human user
    \end{enumerate}
\end{itemize}
The research stated, regarding the \textbf{PageTrackedCount}, that bots tend to behave in two extremes.
Some bots will only submit queries and not browse any of the result pages (except the first one), ostensibly with the intention to increase the query frequency for certain keywords.
The other extreme sees the bots fetch all the result pages for each query, probably trying to reverse engineer the index of the search engine, while genuine human users would probably just browse the first few pages of the query results selectively.
For \textbf{UserClickCount}, clicks on the search engine results, as well as clicks on advertisements within the results, were included in the click counts.
The research noted that the advertisement clicks, though are not distinguished in this work, may include bots specifically targeting ads to click.

With these features, a supervised learning method of bot detection was used.
In said research, the C4.5~\cite{c4.5} algorithm was the decision tree used in lieu of a custom implementation, stating "details of the decision tree algorithm are omitted here".
By leveraging the autonomously created labeled training dataset, sourced from CAPTCHA response and filtered by following the previously mentioned heuristics, the decision tree algorithm was projected to work well.
However, despite the positive projections, the results have shown to be inconsistent from the true data distribution, begging the question of the classification's integrity.
This may be a result of the training dataset generation stage, since the "bot" labels are subject to the heuristics determined by domain experts.
A useful approach to this dilemma was their statistical method of using numerous unlabeled data, due to uncertainties in the "0-cost" training data generation.
Despite the evaluated performance improvement from the tested supervised learning approach, some inconsistencies would need to be addressed in future work.
Due to the limited time of one week to present CAPTCHA challenges to users, some search engine bots may have been undetected, thus diminishing the integrity of the training data generation.
Therefore, it is unclear if this bot detection scheme can be as useful during more real, long-term scenarios.

\section{Similarity Analysis}\label{sec:similarity-analysis}
The bot detection implementation~\cite{bot_detection_wei_alvarez} uses traffic analysis, unsupervised machine learning, removal of duplicate flows, and similarity between malicious and benign traffic flows to provide insight on the botness of a web user.
The research refers to bot web traffic as malicious web traffic and non-malicious web traffic as benign web traffic, which may contain bots that are considered not harmful and are necessary to a system.
Search engine bots, for example, would be categorized as benign web traffic.
A series of clustering algorithms were tested and used to determine which clusters contained the most flows, leading to insight on the characteristics of a bot.
By conducting similarity analysis among these clusters, the work in this research provides a similarity coefficient to describe how malicious traffic data can be distinguished from benign traffic data.

Majority clusters were identified by the number of flows in a cluster.
Since the dataset contained mostly malicious flows, the cluster containing the most flows would also be the malicious flows cluster.
If this was not the case, than the clustering accuracy was therefore to be inaccurate.
Duplicate flows are flows that share the same values for the selected features, a set of networking-related metrics pertaining to packets traveling to and from the webserver.
Similarity between clusters was evaluated using the Jaccard Similarity Coefficient, which was a number ranging from 0 to 1 and the number was the cardinality of the intersection between two clusters, divided by the cardinality of the union between two clusters.

K-means was used to cluster benign and malicious flows, where k = 2.
Although the number of clusters was set known to be two, and the features in these clusters was not biometric data like mouse movement, the method of feature engineering was used as inspiration in this thesis work.
Removal of duplicate flows were shown to make the Jaccard Coefficient less computationally expensive.
However, a large reduction of duplicate flows within a cluster indicated that the cluster contained bots of a botnet.
Detecting anomalies such as this is important to consider when engineering features, a crucial step in the clustering process, in this thesis work.

\section{Outlier Detection}\label{sec:outlier-detection}
There exists a few methods of outlier detection in bot and human user profiles.
Traditional \textbf{statistical outlier detection methods} are univariate.
Such techniques measure a particular atribute in a data distribution, while examining the degree of that value's outlierness.
The parameters, either known or unknown, the number of expected outliers, and the types of expected outliers are the focus of a statistical method.
Commons statistical measures, for example mean and standard deviation, can help find outliers in datasets.
For \textbf{density-based outlier detection methods}, the data points, and their relations to neighbors, are an integral metric to identifying outliers.
By definition, a datapoint is considered an outlier if there aren't many datapoints, or neighbors, near it.
One common algorithm, local outlier factor, measures the density of a datapoint withing a given k-number of datapoint pertaining to the nearest neighbors of a datapoint
Through this approach, outliers are identified as datapoints that have a substantially lower density that its neighbors.
A drawback to this approach, as well as other similar approaches, is that it's only capable of measuring the outlierness of a single datapoint, while it's incapable of identifying clusters of outliers.
Similarly, \textbf{distance-based outlier detection methods} is a method that may apply the local outlier factor.
A key benefit to the distance-based method is its ability to detect single datapoint outliers, as well as clusters of outliers.

The implementation~\cite{particle_swarm} uses outlier detection with a particle swarm optimization algorithm, hierarchical particle swarm based clustering, to detect web bots among human users.
Web bots are said to be examples of outliers since they are able to index a large number of pages in a short amount of time, contrary to human users.
There were two modules included in this work: a clustering module and an outlier detection module.
Both modules work simultaneously to label suspecting outliers, while the clustering module performs clustering in a hierarchical agglomerative manner.
Meanwhile, the outlier detection removes user profiles, that are labeled as suspecting outliers, from succeeding clusters.

This implementation was tested by using a dataset of user profiles that mimic a bots' behavior, as well as dataset without any ground truth, meaning the dataset contained user profiles without labels of their botness.
Three different metrics were used to predict the botness of user profiles: average intra-cluster distance, maximum intra-cluster distance, and the intersection of the average and maximum intra-cluster distances.
The results have shown that, by using the average and maximum intra-cluster distance metrics, bots are detectable when they are "significantly different from [a] legitimate web user" ~\cite{particle_swarm}.

Content goes here~\cite{optimized_outlier_bot_detection}
