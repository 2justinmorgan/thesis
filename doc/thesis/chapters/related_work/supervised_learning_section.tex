
\section{Supervised Learning}\label{sec:supervised-learning}
There are several bot detection schemes that utilize labeled datasets to train models in their implementation.
\subsection{Graphs and CNNs}\label{subsec:graphs-and-cnns}
BotGraph is one which represented the sitemap, or order of page indexing, of a user with a nodes-and-edges graph.
Since these generated graphs are images that represent a number of metrics based on user behavior, not user identification, the research utilized convoluted neural networks (CNN)s to predict users types of either bot or human.
The BotGraph research concludes that this approach yields about a 95\% success rate in detecting bots.

This article begins by introducing key concepts mentioned by a company that specializes in web bot detection, ShieldSquare, the difference of identity-based and behavior-based bot detection.
The identify-based method utilizes client-side JavaScript to collect parameters like browser fingerprints; a collection of information about your browser type and version, as well as your operating system, active plugins, timezone, language, screen resolution and various other active settings~\cite{browser_fingerprinting}.
Whereas the behavior-based refers to the number pages visited per session, the duration of time per page visit, the number of page revisits, etc.
This research more closely resembles the behavior-based method but instead represents such metrics via a graph image which, according to the research, contains more unique features per user.
The paper continues by introducing a violator blacklist, biometric data validation like scroll and mouse movement method by Distil Networks, as well as the UserAgent variable present in HTTP protocol; an unstable bot-detection method as more advanced bots can falsify their identity by simply hiding or modifying the UserAgent variable.
Deep Defense introduced a recurrent neural network (RNN)-based model that takes as input sameshape segment splits from the web server access logs, then encoded the request information in each line of segment to a numerical matrix.
The paper states that, while this method is most similar to the outlined research implementation, the inference efficiency of Deep Defense relies too heavily on the length of the same-shape segments; in which BotGraph supposedly proves to be more stable.

The research implementation included BotGraph, a program that generates graph-based representations of users' sitemap traversals.
Since these graphs were in image format, the implementation employed convolutional neural network (CNN) inferences to distinguish bots from human user types.
The details of BotGraph are as follows:
\begin{itemize}
    \item request - timestamp, HTTP method, request URI, status, host IP, user{\_}agent, client IP variables
    \item session - a method if identifying a series of client requests (by bot or human)
    \item identity - user{\_}agent and client IP variables for the client, and host IP variable for the server
    \item behavior - request URI and status variables for the access frequency metric per graph node.
\end{itemize}
The graph can be described as $G = (V, E)$:
\begin{itemize}
    \item $G$: a directed graph
    \item $V$: set of nodes representing all same-pattern URLs visited, i.e \textbf{/page?id=3} is pattern \textbf{/page?id=*}
    \item $E$: set of directed edges, each representing access points, i.e. mutliple \textbf{a} tag elements with same href
\end{itemize}
Below is a figure of the BotGraph architecture. As you can see, BotGraph runs in a three-step process:
\begin{enumerate}
    \item Build a sitemap through one of three methods: \textit{active crawling, passive sniffing, self providing}
    \begin {enumerate}
        \item \textit{active crawling}: crawling typically starts from website homepage and recursively enters each hyperlink from the current page
        \item \textit{passive sniffing}: the urls of a websiteâ€™s traffic are monitored, learned and then used to build the sitemap. This is a less intrusive alternative to active crawling
        \item \textit{self providing}: the site provides its own sitemap for bot detection. This is the most accurate
    \end{enumerate}
    \item Map requests listed in server access logs to denote sessions as subgraphs in a sitemap
    \item Generate 2-dimensional trace images, translating a bot detection task into an image recognition
\end{enumerate}
\begin{figure}[!h]
    \includegraphics[width=1\columnwidth]{figures/BotGraph_fig1}
    \caption{Architecture of BotGraph}
    \label{fig:botgraph}
\end{figure}
This implementation used a model trained on a data set generated by 30+ professionals that manually tagged web traffic via JavaScript support checking, mouse movement and click tracking, IP reputation, UserAgent blacklisting.
BotGraph is comparable to other bot detection methods such as long short-term memory, SVM, XGBoost, AdaBoost, decision tree (DT), random forest (RF), multi-layer perceptron (MLP), etc.

A weakness in the BotGraph method of bot detection is when a user visits a lower number pages per session, i.e. less than 3 visits.
This is because bot and human users have too similar browsing behavior, namely their session sitemap traversal, creating near-identical sitemap graphs.
However, this research claims that BotGraph is a very effective and efficient method of detecting bots as it achieves about 95\% in precision and recall while relying only on the client's behavior and not the client's identity variables.
Some needed improvements include an implementation that generates more detailed graph-related features to better describe the characteristics of user sessions, specifically to identify the behavior of web bots.

\subsection{Training Data Generation and Semi-Supervised Learning}\label{subsec:training-data-generation-and-semi-supervised-learning}
The~\cite{bot_detection_for_search_engines} research addresses the common problem with supervised learning-based bot detection schemes.
Due to the high traffic of modern search engines, it is infeasible to rely on human judges to generate labeled datasets, used in supervised learning approaches, by manually inspecting the search logs to label bot and human users.
On a controlled webserver environment, labeled datasets were created by analyzing the response and activity of CAPTCHA challenges sent to the users.
In an effort to enhance the user experience, challenges were sent selectively, either when the webserver is experiencing a high volume of network traffic, or when a user makes a high number of requests in a short amount of time.
When presented with a challenge, the user can either disregard the challenge by exiting the session, answer correctly, or answer correctly, thus answering "no response", "correct response", or "wrong response", respectively.
\begin{figure}[!h]
    \centering
    \includegraphics[width=1\columnwidth]{figures/semi_supervised_CAPTCHA_training_data_generation_results}
    \caption{User answers to CAPTCHA challenges}
    \label{fig:captcha-user-answers}
    {\small The research notes that since the users were selected non-uniformly, most answers to the challenge were "no response"}
\end{figure}
About 80\% of the received responses were correct.
This accounted for the majority of the training data with "human" labels.
The remaining "human" labels were pulled from the large set of "no response" answers by analyzing heuristics of the user's number of clicks in a time period, the number of search result pages browsed, as well as information of the user's IP address.
Users were labeled "bot" if the user's answer was "no response", and the user did not satisfy thresholds of the heuristics previously described.
This CAPTCHA challenge-method accounted for the "0-cost" training data generation method, as described in the research.

From the creation of the labeled training dataset, the following features were extracted from the users:
\begin{itemize}
    \item \textbf{PageTrackedCount}: measures the number of pages that the user browses
    \item \textbf{UserClickCount}: measures the number of mouse clicks on the search result pages
    \item \textbf{AllHitCount}: measures the overall "impressions" that the user receives in addition to the search results
    \item \textbf{UserUniqueIPs}: measures the unique number of IPs a user is using
    \item \textbf{UserUniqueQueries}: measures the unique number of queries issued by a single user in a search session
    \item \textbf{Blacklisting Rules}:
    \begin{enumerate}
        \item \textbf{Form}: triggered when a user includes in the query certain obscure codes that are designed mostly for internal search engine instrumentation purposes that should be unfamiliar to most genuine human users
        \item \textbf{IP}: a list of IPs that are publicly identified as Internet spammers and labeled all the traffic from these IPs as "bot"
        \item \textbf{Query}:  this rule is triggered when the query composition is too complicated to be manually typed in by a human user
    \end{enumerate}
\end{itemize}
The research stated, regarding the \textbf{PageTrackedCount}, that bots tend to behave in two extremes.
Some bots will only submit queries and not browse any of the result pages (except the first one), ostensibly with the intention to increase the query frequency for certain keywords.
The other extreme sees the bots fetch all the result pages for each query, probably trying to reverse engineer the index of the search engine, while genuine human users would probably just browse the first few pages of the query results selectively.
For \textbf{UserClickCount}, clicks on the search engine results, as well as clicks on advertisements within the results, were included in the click counts.
The research noted that the advertisement clicks, though are not distinguished in this work, may include bots specifically targeting ads to click.

With these features, a supervised learning method of bot detection was used.
In said research, the C4.5~\cite{c4.5} algorithm was the decision tree used in lieu of a custom implementation, stating "details of the decision tree algorithm are omitted here".
By leveraging the autonomously created labeled training dataset, sourced from CAPTCHA response and filtered by following the previously mentioned heuristics, the decision tree algorithm was projected to work well.
However, despite the positive projections, the results have shown to be inconsistent from the true data distribution, begging the question of the classification's integrity.
This may be a result of the training dataset generation stage, since the "bot" labels are subject to the heuristics determined by domain experts.
A useful approach to this dilemma was their statistical method of using numerous unlabeled data, due to uncertainties in the "0-cost" training data generation.
Despite the evaluated performance improvement from the tested supervised learning approach, some inconsistencies would need to be addressed in future work.
Due to the limited time of one week to present CAPTCHA challenges to users, some search engine bots may have been undetected, thus diminishing the integrity of the training data generation.
Therefore, it is unclear if this bot detection scheme can be as useful during more real, long-term scenarios.
