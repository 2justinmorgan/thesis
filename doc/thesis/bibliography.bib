@misc{fft,
  title={Cal {P}oly {G}ithub},
  note={\url{http://www.github.com/CalPoly}}
}

@misc{thesis_github_repo,
    title = {Justin Morgan's thesis git repo},
    howpublished = {\url{https://github.com/2justinmorgan/thesis}}
}

@article{FEHER201219,
  title = "User identity verification via mouse dynamics",
  journal = "Information Sciences",
  volume = "201",
  pages = "19 - 36",
  year = "2012",
  issn = "0020-0255",
  doi = "https://doi.org/10.1016/j.ins.2012.02.066",
  url = "http://www.sciencedirect.com/science/article/pii/S0020025512001946",
  author = "Clint Feher and Yuval Elovici and Robert Moskovitch and Lior Rokach and Alon Schclar",
  keywords = "Mouse dynamics, Behavioral biometrics, Security monitoring, Verification, Mouse, Pointing devices",
  abstract = "Identity theft is a crime in which hackers perpetrate fraudulent activity under stolen identities by using credentials, such as passwords and smartcards, unlawfully obtained from legitimate users or by using logged-on computers that are left unattended. User verification methods provide a security layer in addition to the username and password by continuously validating the identity of logged-on users based on their physiological and behavioral characteristics. We introduce a novel method that continuously verifies users according to characteristics of their interaction with the mouse. The contribution of this work is threefold: first, user verification is derived based on the classification results of each individual mouse action, in contrast to methods which aggregate mouse actions. Second, we propose a hierarchy of mouse actions from which the features are extracted. Third, we introduce new features to characterize the mouse activity which are used in conjunction with features proposed in previous work. The proposed algorithm outperforms current state-of-the-art methods by achieving higher verification accuracy while reducing the response time of the system."
}

@inproceedings{1ee426975c3d46d2ba6ef5c2d76384c5,
  title = "Detecting and characterizing web bot traffic in a large e-commerce marketplace",
  abstract = "A certain amount of web traffic is attributed to web bots on the Internet. Web bot traffic has raised serious concerns among website operators, because they usually consume considerable resources at web servers, resulting in high workloads and longer response time, while not bringing in any profit. Even worse, the content of the pages it crawled might later be used for other fraudulent activities. Thus, it is important to detect web bot traffic and characterize it. In this paper, we first propose an efficient approach to detect web bot traffic in a large e-commerce marketplace and then perform an in-depth analysis on the characteristics of web bot traffic. Specifically, our proposed bot detection approach consists of the following modules: (1) an Expectation Maximization (EM)-based feature selection method to extract the most distinguishable features, (2) a gradient based decision tree to calculate the likelihood of being a bot IP, and (3) a threshold estimation mechanism aiming to recover a reasonable amount of non-bot traffic flow. The detection approach has been applied on Taobao/Tmall platforms, and its detection capability has been demonstrated by identifying a considerable amount of web bot traffic. Based on data samples of traffic originating from web bots and normal users, we conduct a comparative analysis to uncover the behavioral patterns of web bots different from normal users. The analysis results reveal their differences in terms of active time, search queries, item and store preferences, and many other aspects. These findings provide new insights for public websites to further improve web bot traffic detection for protecting valuable web contents.",
  author = "Haitao Xu and Zhao Li and Chen Chu and Yuanmi Chen and Yifan Yang and Haifeng Lu and Haining Wang and Angelos Stavrou",
  year = "2018",
  month = jan,
  day = "1",
  doi = "10.1007/978-3-319-98989-1_8",
  language = "English (US)",
  isbn = "9783319989884",
  series = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer Verlag",
  pages = "143--163",
  editor = "Jianying Zhou and Miguel Soriano and Javier Lopez",
  booktitle = "Computer Security - 23rd European Symposium on Research in Computer Security, ESORICS 2018, Proceedings",
  note = "23rd European Symposium on Research in Computer Security, ESORICS 2018 ; Conference date: 03-09-2018 Through 07-09-2018",
}

@article{ROVETTA2020102577,
  title = "Bot recognition in a Web store: An approach based on unsupervised learning",
  journal = "Journal of Network and Computer Applications",
  volume = "157",
  pages = "102577",
  year = "2020",
  issn = "1084-8045",
  doi = "https://doi.org/10.1016/j.jnca.2020.102577",
  url = "http://www.sciencedirect.com/science/article/pii/S1084804520300515",
  author = "Stefano Rovetta and Gra≈ºyna Suchacka and Francesco Masulli",
  keywords = "Web bot, Internet robot, Web bot detection, Supervised classification, Unsupervised classification, Machine learning, Web server",
  abstract = "Web traffic on e-business sites is increasingly dominated by artificial agents (Web bots) which pose a threat to the website security, privacy, and performance. To develop efficient bot detection methods and discover reliable e-customer behavioural patterns, the accurate separation of traffic generated by legitimate users and Web bots is necessary. This paper proposes a machine learning solution to the problem of bot and human session classification, with a specific application to e-commerce. The approach studied in this work explores the use of unsupervised learning (k-means and Graded Possibilistic c-Means), followed by supervised labelling of clusters, a generative learning strategy that decouples modelling the data from labelling them. Its efficiency is evaluated through experiments on real e-commerce data, in realistic conditions, and compared to that of supervised learning classifiers (a multi-layer perceptron neural network and a support vector machine). Results demonstrate that the classification based on unsupervised learning is very efficient, achieving a similar performance level as the fully supervised classification. This is an experimental indication that the bot recognition problem can be successfully dealt with using methods that are less sensitive to mislabelled data or missing labels. A very small fraction of sessions remain misclassified in both cases, so an in-depth analysis of misclassified samples was also performed. This analysis exposed the superiority of the proposed approach which was able to correctly recognize more bots, in fact, and identified more camouflaged agents, that had been erroneously labelled as humans."
}

@article{bad_bot_report,
  title = {2019 BAD BOT REPORT: The Bot Arms Race Continues},
  url = {https://www.bluecubesecurity.com/wp-content/uploads/bad-bot-report-2019LR.pdf},
  author = {Distil Networks},
  year = {2019},
}

@article{Hamidzadeh2018,
   author={Hamidzadeh, Javad and Zabihimayvan, Mahdieh and Sadeghi, Reza},
   title={Detection of Web site visitors based on fuzzy rough sets},
   journal={Soft Computing},
   year={2018},
   month={Apr},
   day={01},
   volume={22},
   number={7},
   pages={2175-2188},
   abstract={Despite emerging of Web 2.0 applications and increasing requirements to well-behaved Web robots, malicious ones can reveal irreparable risks for Web sites. Regardless of behavior of Web robots, they may occupy bandwidth and reduce performance of Web servers. In spite of many prestigious researches trying to characterize Web visitors and classify them, there is a lack of concentration on feature selection to dynamically choose attributes used to describe Web sessions. On the other hand, depending on an accurate clustering technique, which can deal with huge number of samples in a reasonable amount of time, is practically important. Therefore, in this paper, a new algorithm, fuzzy rough set--Web robot detection (FRS-WRD), is proposed based on fuzzy rough set theory to better characterize and cluster Web visitors of three real Web sites. External evaluations show that in contrast to state-of-the-art algorithms, FRS-WRD achieves better results in terms of G-mean 95{\%}, Jaccard 88{\%}, entropy 0.36, and finally, purity 96{\%}. Moreover, according to confusion matrixes, it can better detect malicious Web visitors.},
   issn={1433-7479},
   doi={10.1007/s00500-016-2476-4},
   url={https://doi.org/10.1007/s00500-016-2476-4}
}

@article{ZABIHIMAYVAN2017129,
    title = "A soft computing approach for benign and malicious web robot detection",
    journal = "Expert Systems with Applications",
    volume = "87",
    pages = "129 - 140",
    year = "2017",
    issn = "0957-4174",
    doi = "https://doi.org/10.1016/j.eswa.2017.06.004",
    url = "http://www.sciencedirect.com/science/article/pii/S0957417417304116",
    author = "Mahdieh Zabihimayvan and Reza Sadeghi and H. Nathan Rude and Derek Doran",
    keywords = "Markov clustering algorithm, Web Robot Detection, Web crawler, Malicious web agents, Fuzzy Rough Set Theory",
    abstract = "The accurate detection of web robot sessions from a web server log is essential to take accurate traffic-level measurements and to protect the performance and privacy of information on a Web server. Moreover, the irrecoverable risks of visits from malicious robots that intentionally try to evade web server intrusion detection systems, covering-up their visits with fabricated fields in their http request packets, cannot be ignored. To separate both types of robots from humans in practice, analysts turn to heuristic methods or state-of-the-art soft computing approaches that have only been tuned to the specification of a kind of web server. Noting that the landscape of web robot agents is ever changing, and that behavioral patterns and characteristics vary across different web servers, both options are lacking. To overcome this challenge, this paper presents SMART, a soft computing system that simultaneously detects benign and malicious types of robot agents from web server logs and can automatically adapt to the session characteristics of a web server. The results of experiments over some access log file servers, each servicing different domains of the web, demonstrate outperformance of the proposed method on state-of-the-art ones for benign and malicious robot detection."
}

@article{akamai_bot_detection,
    title = {Akamai‚Äôs Bot Manager - Advanced strategies to flexibly manage the long-term business and IT impact of bots},
    author = {Akamai},
    url = {https://www.akamai.com/us/en/multimedia/documents/product-brief/bot-manager-product-brief.pdf},
    year = {2018}
}

@inproceedings{10.1145/3339252.3339267,
    author = {Iliou, Christos and Kostoulas, Theodoros and Tsikrika, Theodora and Katos, Vasilis and Vrochidis, Stefanos and Kompatsiaris, Yiannis},
    title = {Towards a Framework for Detecting Advanced Web Bots},
    year = {2019},
    isbn = {9781450371643},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3339252.3339267},
    doi = {10.1145/3339252.3339267},
    abstract = {Automated programs (bots) are responsible for a large percentage of website traffic. These bots can either be used for benign purposes, such as Web indexing, Website monitoring (validation of hyperlinks and HTML code), feed fetching Web content and data extraction for commercial use or for malicious ones, including, but not limited to, content scraping, vulnerability scanning, account takeover, distributed denial of service attacks, marketing fraud, carding and spam. To ensure their security, Web servers try to identify bot sessions and apply special rules to them, such as throttling their requests or delivering different content. The methods currently used for the identification of bots are based either purely on rule-based bot detection techniques or a combination of rule-based and machine learning techniques. While current research has developed highly adequate methods for Web bot detection, these methods' adequacy when faced with Web bots that try to remain undetected hasn't been studied. For this reason, we created and evaluated a Web bot detection framework on its ability to detect conspicuous bots separately from its ability to detect advanced Web bots. We assessed the proposed framework performance using real HTTP traffic from a public Web server. Our experimental results show that the proposed framework has significant ability to detect Web bots that do not try to hide their bot identity using HTTP Web logs (balanced accuracy in a false-positive intolerant server > 95%). However, detecting advanced Web bots that present a browser fingerprint and may present a humanlike behaviour as well is considerably more difficult.},
    booktitle = {Proceedings of the 14th International Conference on Availability, Reliability and Security},
    articleno = {18},
    numpages = {10},
    keywords = {Web bot detection, humanlike behaviour, Advanced Web bots, Evasive Web bots},
    location = {Canterbury, CA, United Kingdom},
    series = {ARES '19}
}

@article{STEVANOVIC2013698,
    title = "Detection of malicious and non-malicious website visitors using unsupervised neural network learning",
    journal = "Applied Soft Computing",
    volume = "13",
    number = "1",
    pages = "698 - 708",
    year = "2013",
    issn = "1568-4946",
    doi = "https://doi.org/10.1016/j.asoc.2012.08.028",
    url = "http://www.sciencedirect.com/science/article/pii/S1568494612003778",
    author = "Dusan Stevanovic and Natalija Vlajic and Aijun An",
    keywords = "Web crawler detection, Neural networks, Web server access logs, Machine learning, Clustering, Denial of service",
    abstract = "Distributed denials of service (DDoS) attacks are recognized as one of the most damaging attacks on the Internet security today. Recently, malicious web crawlers have been used to execute automated DDoS attacks on web sites across the WWW. In this study, we examine the use of two unsupervised neural network (NN) learning algorithms for the purpose web-log analysis: the Self-Organizing Map (SOM) and Modified Adaptive Resonance Theory 2 (Modified ART2). In particular, through the use of SOM and modified ART2, our work aims to obtain a better insight into the types and distribution of visitors to a public web-site based on their browsing behavior, as well as to investigate the relative differences and/or similarities between malicious web crawlers and other non-malicious visitor groups. The results of our study show that, even though there is a pretty clear separation between malicious web-crawlers and other visitor groups, 52% of malicious crawlers exhibit very ‚Äòhuman-like‚Äô browsing behavior and as such pose a particular challenge for future web-site security systems. Also, we show that some of the feature values of malicious crawlers that exhibit very ‚Äòhuman-like‚Äô browsing behavior are not significantly different than the features values of human visitors. Additionally, we show that Google, MSN and Yahoo crawlers exhibit distinct crawling behavior."
}

@inproceedings{10.1109/DSN.2013.6575366,
    author = {Jin, Jing and Offutt, Jeff and Zheng, Nan and Mao, Feng and Koehl, Aaron and Wang, Haining},
    title = {Evasive Bots Masquerading as Human Beings on the Web},
    year = {2013},
    isbn = {9781467364713},
    publisher = {IEEE Computer Society},
    address = {USA},
    url = {https://doi.org/10.1109/DSN.2013.6575366},
    doi = {10.1109/DSN.2013.6575366},
    abstract = {Web bots such as crawlers are widely used to automate various online tasks over the Internet. In addition to the conventional approach of human interactive proofs such as CAPTCHAs, a more recent approach of human observational proofs (HOP) has been developed to automatically distinguish web bots from human users. Its design rationale is that web bots behave intrinsically differently from human beings, allowing them to be detected. This paper escalates the battle against web bots by exploring the limits of current HOP-based bot detection systems. We develop an evasive web bot system based on human behavioral patterns. Then we prototype a general web bot framework and a set of flexible de-classifier plugins, primarily based on application-level event evasion. We further abstract and define a set of benchmarks for measuring our system's evasion performance on contemporary web applications, including social network sites. Our results show that the proposed evasive system can effectively mimic human behaviors and evade detectors by achieving high similarities between human users and evasive bots.},
    booktitle = {Proceedings of the 2013 43rd Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)},
    pages = {1‚Äì12},
    numpages = {12},
    keywords = {human observation proofs, Servers, machine learning, Web security, bot},
    series = {DSN '13}
}

@inproceedings{inproceedings,
    author = {Gu, Guofei and Perdisci, Roberto and Zhang, Junjie and Lee, Wenke},
    year = {2008},
    month = {01},
    pages = {139-154},
    title = {BotMiner: Clustering Analysis of Network Traffic for Protocol- and Structure-Independent Botnet Detection},
    journal = {CCS'08}
}

@article{bot_detection_wei_alvarez,
    author = {Wu, Wei and Alvarez, Jaime and Liu, Chengcheng and Sun, Hung-Min},
    year = {2018},
    month = {01},
    pages = {},
    title = {Bot detection using unsupervised machine learning},
    volume = {24},
    journal = {Microsystem Technologies},
    doi = {10.1007/s00542-016-3237-0}
}

@article{botonus,
    author = {Yahyazadeh, Mosa and Abadi, Mahdi},
    year = {2012},
    month = {01},
    pages = {51‚Äì62},
    title = {BotOnus: An online unsupervised method for botnet detection},
    volume = {4},
    journal = {ISeCure: The ISC International Journal of Information Security}
}

@article{particle_swarm,
    author = {Alam, Shafiq and Dobbie, Gillian and Koh, Yun Sing and Riddle, Patricia},
    year = {2014},
    month = {09},
    pages = {2955-2962},
    title = {Web bots detection using Particle Swarm Optimization based clustering},
    journal = {Proceedings of the 2014 IEEE Congress on Evolutionary Computation, CEC 2014},
    doi = {10.1109/CEC.2014.6900644}
}

@article{optimized_outlier_bot_detection,
    author = {R. Peter and D. Divya},
    title = {Optimized Outlier Based Web Bot Detection},
    journal = {Journal of Network and Information Security},
    volume = {5},
    number = {1},
    year = {2017},
    keywords = {Clustering, Optimization, Outlier, PSO.},
    abstract = {By the turn of century, the use of computers and accessing internet were rapidly increases. As the increasing the network access it increases the network attacks also. The nature of attacks may vary in each day. Today‚Äôs trends of attacks are web bots. Web bots can be used for both useful and destructive purposes. Now a day‚Äôs attackers use bot nets for malicious intents. Bots are basically a computer program that surf multiple websites without the intention of the user to perform variety of tasks. If any web bots were present in network it may distort the analysis process which leads to incorrect pattern and cause wrong decision making. The web bots requests were different from genuine request. So it can consider web bots are example of outliers and detect them using outlier detection methods. In this project use Swarm Intelligent (SI) based technique called Particle Swarm Optimization technique (PSO) for detect outliers or web bots. The efficiency of PSO algorithm depends on its parameters. For improving the efficiency of PSO algorithm it need some changes in its parameters. So for improving the efficiency of outlier detection optimization based HPSO (Hierarchical Particle Swarm Optimization) algorithm were used.},
    url = {http://www.i-scholar.in/index.php/jnis/article/view/159736}
}

@article{human_computer_interaction_based_intrusion_detection,
    author={R. V. {Yampolskiy}},
    booktitle={Fourth International Conference on Information Technology (ITNG'07)},
    title={Human Computer Interaction Based Intrusion Detection},
    year={2007},
    pages={837-842},
    doi={10.1109/ITNG.2007.101}
}

@article{7371507,
    author={Y. {Yang} and N. {Vlajic} and U. T. {Nguyen}},
    booktitle={2015 IEEE 2nd International Conference on Cyber Security and Cloud Computing},
    title={Next Generation of Impersonator Bots: Mimicking Human Browsing on Previously Unvisited Sites},
    year={2015},
    volume={},
    number={},
    pages={356-361},
    doi={10.1109/CSCloud.2015.93}
}

@article{intrustion_detection_using_mouse_dynamics,
    author = {Antal, Margit and Egyed-Zsigmond, Elod},
    year = {2018},
    month = {10},
    pages = {},
    title = {Intrusion Detection Using Mouse Dynamics}
}

@article{how_recaptcha_is_improving_user_experience,
    author = {Rebecca from Entyce},
    year = {2019},
    pages = {},
    title = {How reCAPTCHA in Improving User Experience}
}

@article{mouse_dpi_and_usb_polling_rate,
    author = {Coding Horror},
    title = {Mouse DPI and USB Polling Rate},
    year = {2007}
}

@article{mouse_dpi_and_polling_rate_explained,
    author = {Chris Hoffman},
    title = {Mouse DPI and Polling Rates Explained: Do They Matter for Gaming?},
    year = {2017},
}

@misc{balabit_dataset,
    title = {Balabit Mouse Challenge Dataset},
    howpublished = {\url{https://github.com/balabit/Mouse-Dynamics-Challenge}},
    note = {Accessed: 2021-02-04}
}

@article{botgraph,
    author = {Luo, Yang and She, Guozhen and Huang, Jinwan and Cheng, Peng and Xiong, Yongqiang},
    year = {2019},
    month = {03},
    pages = {},
    title = {BotGraph: Web Bot Detection Based on Sitemap}
}

@misc{browser_fingerprinting,
    author = {Chris Hauk},
    title = {Browser Fingerprinting: What Is It And What Should You Do About It?},
    year = {2021},
    url = {https://pixelprivacy.com/resources/browser-fingerprinting/},
}

@inproceedings{bot_detection_for_search_engines,
    author = {Kang, Hongwen and Wang, Kuansan and Soukal, David and Behr, Fritz and Zheng, Zijian},
    title = {Large-Scale Bot Detection for Search Engines},
    year = {2010},
    isbn = {9781605587998},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/1772690.1772742},
    doi = {10.1145/1772690.1772742},
    abstract = {In this paper, we propose a semi-supervised learning approach for classifying program (bot) generated web search traffic from that of genuine human users. The work is motivated by the challenge that the enormous amount of search data pose to traditional approaches that rely on fully annotated training samples. We propose a semi-supervised framework that addresses the problem in multiple fronts. First, we use the CAPTCHA technique and simple heuristics to extract from the data logs a large set of training samples with initial labels, though directly using these training data is problematic because the data thus sampled are biased. To tackle this problem, we further develop a semi-supervised learning algorithm to take advantage of the unlabeled data to improve the classification performance. These two proposed algorithms can be seamlessly combined and very cost efficient to scale the training process. In our experiment, the proposed approach showed significant (i.e. 2:1) improvement compared to the traditional supervised approach.},
    booktitle = {Proceedings of the 19th International Conference on World Wide Web},
    pages = {501‚Äì510},
    numpages = {10},
    keywords = {click logs, search engine, query logs, captcha, semi-supervised learning, bot detection},
    location = {Raleigh, North Carolina, USA},
    series = {WWW '10}
}

@book{c4.5,
    author = {Quinlan, J. Ross},
    title = {C4.5: Programs for Machine Learning},
    year = {1993},
    isbn = {1558602380},
    publisher = {Morgan Kaufmann Publishers Inc.},
    address = {San Francisco, CA, USA}
}

@InProceedings{deep_learning_detection_with_mouse_behavior,
    author="Wei, Ang and Zhao, Yuxuan and Cai, Zhongmin",
    editor="Sun, Zhenan and He, Ran and Feng, Jianjiang and Shan, Shiguang and Guo, Zhenhua",
    title="A Deep Learning Approach to Web Bot Detection Using Mouse Behavioral Biometrics",
    booktitle="Biometric Recognition",
    year="2019",
    publisher="Springer International Publishing",
    address="Cham",
    pages="388--395",
    abstract="Web bots are automated scripts that perform online tasks like human. Abuse of bot technology poses various threats to the security of websites. Recently, mouse dynamics has been applied to bot detection by analyzing whether recorded mouse operations are consistent with human operational patterns. In this paper, we introduce a deep neural network approach to bot detection. We propose a new representation method for mouse movement data, which converts every mouse movement into an image containing its spatial and kinematic information. This representation method makes it possible to utilize CNN models to automate feature learning from mouse movement data. Experimental results demonstrate that our method is able to detect 96.2{\%} of bots with statistical attack ability while traditional detection methods using hand-crafted features or RNN can only detect less than 30{\%} of them.",
    isbn="978-3-030-31456-9"
}

@Inbook{bot_or_human,
    author="Chu, Zi and Gianvecchio, Steven and Wang, Haining",
    editor="Samarati, Pierangela and Ray, Indrajit and Ray, Indrakshi",
    title="Bot or Human? A Behavior-Based Online Bot Detection System",
    bookTitle="From Database to Cyber Security: Essays Dedicated to Sushil Jajodia on the Occasion of His 70th Birthday",
    year="2018",
    publisher="Springer International Publishing",
    address="Cham",
    pages="432--449",
    abstract="The abuse of Internet online services by automated programs, known as bots, poses a serious threat to Internet users. Bots target popular Internet online services, such as web blogs and online social networks, to distribute spam and malware. In this work, we will first characterize the human behaviors and bot behaviors in online services. Based on the behavior characterization, we propose an effective detection system to accurately distinguish bots from humans. Our proposed detection system consists of two main components: (1) a client-side logger and (2) a server-side classifier. The client-side logger records user behavioral events such as mouse movement and keystroke data, and provides this data in batches to a server-side classifier which identifies a user as human or bot. Our experimental results demonstrate that our proposed detection is able to achieve very high accuracy with negligible overhead.",
    isbn="978-3-030-04834-1",
    doi="10.1007/978-3-030-04834-1_21",
    url="https://doi.org/10.1007/978-3-030-04834-1_21"
}
